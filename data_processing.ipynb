{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "lags = 12\n",
    "train = \"/Users/marleywetini/repos/intelligentSystems/data/Scats Data October 2006.csv\"\n",
    "test = \"/Users/marleywetini/repos/intelligentSystems/data/SCAT Test Data.xlsx\"\n",
    "\n",
    "#used to get split as data for test needs to be a linear timeline for an accurate visual representation.\n",
    "def custom_train_test_split(data):\n",
    "    size = -504  # Use the last 504 rows for training/testing\n",
    "    target_col = 14  # This is the column index you want to predict\n",
    "    # Create x_train and x_test by dropping the target column (features)\n",
    "    x_train = np.delete(data[:size],target_col, axis=1)  # Take all rows except the last 504 for training\n",
    "    x_test = np.delete(data[size:], target_col, axis=1)   # Take the last 504 rows for testing\n",
    "\n",
    "    # Create y_train and y_test (target) from the target column\n",
    "    y_train = data[:size, target_col]  # Target for training\n",
    "    y_test = data[size:, target_col]   # Target for testing\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "# Step 1: Load and clean the data\n",
    "def process_flow_data(flow_data, max_len=None):\n",
    "    \"\"\"\n",
    "    This function processes the flow data by flattening and padding/truncating to ensure uniform length.\n",
    "    \"\"\"\n",
    "    # Flatten the lists inside each flow entry and find the maximum length if not provided\n",
    "    flattened_flow_data = [np.array(x).flatten() for x in flow_data]\n",
    "    if max_len is None:\n",
    "        max_len = max(len(arr) for arr in flattened_flow_data)  # Find the max length of any array\n",
    "    \n",
    "    # Pad or truncate each array to the max length\n",
    "    padded_flow_data = np.array([np.pad(arr, (0, max_len - len(arr)), 'constant') \n",
    "                                 if len(arr) < max_len else arr[:max_len] \n",
    "                                 for arr in flattened_flow_data])\n",
    "    return padded_flow_data\n",
    "\n",
    "# Step 1: Load and clean the data\n",
    "df = pd.read_csv(train, header=1).fillna(0)\n",
    "# Step 2: Define the flow columns (V00 to V95 for 96 time intervals)\n",
    "flow_columns = [f\"V{str(i).zfill(2)}\" for i in range(96)]\n",
    "grouped_data = df.groupby(['NB_LATITUDE', 'NB_LONGITUDE'])[flow_columns].apply(lambda x: x.values.tolist())\n",
    "flow_data = grouped_data.values # shape=(140,0) | pandas\n",
    "flow_data = np.array(flow_data) # shape=(140,0) | numpy.ndarray\n",
    "\n",
    "#new_flow_data is used to allocate the Min/Max for data scaler, needs to be do before the for loop to ensure that min/max is set for entire dataset\n",
    "new_flow_data = process_flow_data(flow_data)\n",
    "data_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaler = data_scaler.fit(new_flow_data.reshape(-1, 1))\n",
    "latlong_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "latlong_data = grouped_data.reset_index()[['NB_LATITUDE', 'NB_LONGITUDE']]\n",
    "\n",
    "#Transform lat/long data, do before for loop to ensure its min/mnax is based on entire array\n",
    "latlong_scaler = MinMaxScaler(feature_range=(0, 1)).fit(latlong_data)\n",
    "latlong_scaled = latlong_scaler.transform(latlong_data)\n",
    "\n",
    "train_data = []\n",
    "for i, flow in enumerate(flow_data):\n",
    "    flow = np.array(flow).flatten()\n",
    "    flow = data_scaler.transform(flow.reshape(-1,1)).reshape(1, -1)\n",
    "    for j in range(0, len(flow[0]) - lags + 1):  # Iterating over each possible lag of 12\n",
    "        lagged_flow = flow[0][j:j+lags + 1]  # Get the flow data for a lag of 12\n",
    "        # Attach corresponding latlong data\n",
    "        latlong = latlong_scaled[i]  # Get the lat/long for the current location //# to check latlong/flow data is correctly being added. \n",
    "        # Combine latlong and flow data\n",
    "        combined_arr = np.hstack((latlong, lagged_flow))\n",
    "        # Append the combined array to the training data\n",
    "        train_data.append(combined_arr)\n",
    "\n",
    "train_data = process_flow_data(train_data) #numpy.ndarray | an array of arrays.\n",
    "X_train=[]\n",
    "X_test=[]\n",
    "Y_train=[]\n",
    "Y_test=[]\n",
    "for i in range(0, len(latlong_scaled)):\n",
    "    # Create a boolean mask where both the latitude and longitude match\n",
    "    mask = (train_data[:, 0] == latlong_scaled[i][0]) & (train_data[:, 1] == latlong_scaled[i][1])\n",
    "    \n",
    "    # Select the rows where the mask is True (i.e., matching lat/long pairs)\n",
    "    df = train_data[mask]\n",
    "    x_train, x_test,y_train,y_test=custom_train_test_split(df)\n",
    "\n",
    "    X_train.append(x_train)\n",
    "    X_test.append(x_test)\n",
    "    Y_train.append(y_train)\n",
    "    Y_test.append(Y_test)\n",
    "\n",
    "X_train = process_flow_data(X_train)\n",
    "X_test = np.array(x_test)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "print(X_train)\n",
    "print(X_test.shape)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Summary:\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Dense)             (None, 10)                130       \n",
      "                                                                 \n",
      " activation_72 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " Decoder (Dense)             (None, 12)                132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 262 (1.02 KB)\n",
      "Trainable params: 262 (1.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model 2 Summary:\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " activation_73 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " Decoder (Dense)             (None, 12)                108       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196 (784.00 Byte)\n",
      "Trainable params: 196 (784.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model 3 Summary:\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " activation_74 (Activation)  (None, 4)                 0         \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 4)                 0         \n",
      "                                                                 \n",
      " Decoder (Dense)             (None, 12)                60        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96 (384.00 Byte)\n",
      "Trainable params: 96 (384.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model 4 Summary:\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden1 (Dense)             (None, 10)                130       \n",
      "                                                                 \n",
      " activation_75 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      " hidden2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " activation_76 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " hidden3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " activation_77 (Activation)  (None, 4)                 0         \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 259 (1.01 KB)\n",
      "Trainable params: 259 (1.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "\n",
    "def _get_sae(inputs, hidden, output):\n",
    "    \"\"\"SAE(Auto-Encoders)\n",
    "    Build SAE Model.\n",
    "\n",
    "    # Arguments\n",
    "        inputs: Integer, number of input units.\n",
    "        hidden: Integer, number of hidden units.\n",
    "        output: Integer, number of output units.\n",
    "    # Returns\n",
    "        model: Model, nn model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden, input_dim=inputs, name='encoder'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output, name='Decoder', activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_saes(layers):\n",
    "    \"\"\"SAEs(Stacked Auto-Encoders)\n",
    "    Build SAEs Model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: List(int), number of input, output and hidden units.\n",
    "    # Returns\n",
    "        models: List(Model), List of SAE and SAEs.\n",
    "    \"\"\"\n",
    "    \n",
    "    sae1 = _get_sae(layers[0], layers[1], layers[0])\n",
    "    sae2 = _get_sae(layers[1], layers[2], layers[0])\n",
    "    sae3 = _get_sae(layers[2], layers[3], layers[0])\n",
    "\n",
    "    saes = Sequential()\n",
    "    saes.add(Dense(layers[1], input_dim=layers[0], name='hidden1'))\n",
    "    saes.add(Activation('relu'))\n",
    "    saes.add(Dense(layers[2], name='hidden2'))\n",
    "    saes.add(Activation('relu'))\n",
    "    saes.add(Dense(layers[3], name='hidden3'))\n",
    "    saes.add(Activation('relu'))\n",
    "    saes.add(Dropout(0.2))\n",
    "    saes.add(Dense(layers[4], activation='sigmoid'))\n",
    "\n",
    "    models = [sae1, sae2, sae3, saes]\n",
    "    return models\n",
    "\n",
    "layers= [12, 10, 8, 4, 1]\n",
    "models = get_saes(layers)\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    print(f\"Model {idx + 1} Summary:\")\n",
    "    model.summary()\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
